+++
date = "2022-10-10"
title = "linear mixed models: what and why"
mathjax= true
categories = [ "posts", "learning notes", "explain to me like i'm five" ]
tags = [ "statistics", "data science" ]
+++

## what is a statistical model?

statistical modeling is the process of applying statistical analysis to a dataset ([1]). a good definition is: **how could we generalize the relationship such that we no longer need measurement at every single point?** 

## linear modeling

for example, if i want to model weight vs. height (heigt predicts weight), i collect and visualize the data on a scatterplot, then draw a line that would fit these datapoints to the degree that i don't need "every single weight to every single height". this means here i am performing stats model that simply by examining the line, i could understand the relationship between the two as well as extrapolate my data to predict the other variables. this is called linear modeling, drawing a straight, best-fitted line between the two variables.

## non-linear model

however, it is not so simple in the real world. take administering drug doses and examining the percentage of growth relative to control- it's a logarithmic relationship rather than linear because sometimes the growth after drug administration spikes up. so for drug toxicity, performing titration, etc, we have different kinds of curve fitting techniques.

in terms of figuring out which model to use on which system, this requires you to understand the data. the easiest way is to plot the data on 2D or find some literature to see what others did, then apply that to your own data.

## linear model with multiple x

"a multiple linear regression model shows the relationship between the dependent variable and multiple (two or more) independent variables."([2]) for example, i want to have a model that predicts the probability to rain tomorrow, i could have GPS location, wind speed, the weather 5 days beforehand, sea level, blah, blah blah. so instead of just one line, you’re depicting a hyperplane between multiple variables, and so on.

here, you can't run a simple linear model because some of the predictor variables are categorical, i.e. rain or no rain yesterday (y/n)? for this we might use a logistic regression, which is a part of what's called the **general linear model (GLM)**-- a multivariate linear model where one or few of the variate is a non-linear regression.

anyway the point is not everything is linear, so if you were to model the relationship with all these variables, some linear, some not linear, then you run a model that generalizes all that.

## general linear model (GLM)

for [example]: do age, gender, ethnicity, and going out to eat frequently all affect cholesterol levels?

***dependent variable:*** chlesterol level

***independent variables:***
- age(years)
- gender(male/female)
- race/ethnicity (black, white, asian, hispanic)
- frequency of going out to eat (5+ times/week vs. less than 5 times/week)

if you run all this in a GLM model you will have a hard time. first how are you going to define race? you need something else.

## linear mixed models: 

we need a linear mixed model. this just means we're modelling correlated data. here we have multiple inputs for the model, but some of the inputs are **non-independent** from each other.

it's called a linear ***mixed*** model because it models two types of effects: fixed and random. if you know something that would directly affect the prediction, it's a fixed effect. but a random effect is the variation between the individual predictor variable. 

- **so fixed effects represent population-level (i.e., average) effects that should persist across experiments.**

- **whereas fixed effects model average trends, random effects model the extent to which these trends vary across levels of some grouping factor (e.g., participants or items)**

Again, a fixed effect is an average effect across all clusters and a random effect is a variance that describes how much an effect differs across clusters. Generally, fixed effects in MLMs capture the mean of an effect and the random effect captures the variance of an effect.

if you want to quantify the variation by averging them you might not end up getting the full picture of what is happening, because the *variation* between each individual subject is just too big.

so for another example: the effect of a drug dose to certain individual. you give your drug to all individuals and you're meausre five different dose responses. of course, you could see that if you increase the concentration of the drug, it would kill the individuals more; however, the degree to which it kills differs wildly among the differnet people.

this is called the **batch effect** or the simpson's paradox ([3]). you could also see that the pearson’s correlation, or the power of the statistical correlation is way lower. most importantly, you may end up reaching a completely different conclusion (type 1 error).

LMM assumptions:
- responses are normally distributed, independent, and equal variance *conditional* on random effects
- random effects: normally distributed, independent, and mean zero (not necessarily same variance, i.e. we could one variation for each random variable)

additional notes: what if our observations are non-normal and correlated?
use a "generalized linear mixed model" (GLMM)
- incorprate random effects to include correlation
- model log odds or log mean

linear mixed models assume your response (or dependent) variable is normally distributed. generalized linear mixed models do not.

see: https://www.youtube.com/watch?v=RCZEwkaLsJM&ab_channel=ChristinaKnudson

## the sin of pseudoreplication

this is exactly what we want to avoid. here mixed models could help us avoid the sin of pseudoreplication which is a source of error in statistical inference where we treat the data as being independent when it is not.

pseudoreplication typically occurs either in an observational study with a hierarchical structure or a designed experiment with differing spatial and/or temporal scales.

so if you do a linear mixed model you could increase the statistical power, and make the results more generalizable.

## resources
1. https://docs.google.com/presentation/d/1koQM0SNB1AS2LGwRpHg1kqiHRIIfJyYjD7G-meUm8kc/edit#slide=id.g110c872cb1d_1_4

2. https://jontalle.web.engr.illinois.edu/MISC/lme4/bw_LME_tutorial.pdf

3. https://towardsdatascience.com/how-linear-mixed-model-works-350950a82911

[tutorial]: https://docs.google.com/presentation/d/1koQM0SNB1AS2LGwRpHg1kqiHRIIfJyYjD7G-meUm8kc/edit#slide=id.g110c872cb1d_1_4
[1]: https://www.northeastern.edu/graduate/blog/statistical-modeling-for-data-analysis/
[2]: http://www.aetheling.com/models/cusp/Intro.htm
[example]: https://blackboard.jhu.edu/bbcswebdav/pid-3918442-dt-content-rid-17483493_2/courses/NR.120.508.0101.SP17/120.508%20Module%208%20Multiple%20Regression%20%28PDF%20Full%20page%20color%29.pdf
[3]:https://towardsdatascience.com/how-linear-mixed-model-works-350950a82911